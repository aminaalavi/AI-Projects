{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==== Hybrid LangGraph: agents for intelligence, Python for execution ==========\n",
        "# What this does:\n",
        "# 1) schema  -> read both DB schemas (Python)\n",
        "# 2) mapplan -> LLM proposes mapping JSON (fallback to your heuristic)\n",
        "# 3) plan    -> deterministic plan from mapping (Python)\n",
        "# 4) sqlgen  -> deterministic SQL from plan (Python)\n",
        "# 5) exec    -> execute SQL (Python; captures errors)\n",
        "# 6) validate-> counts + checksum (Python)\n",
        "# 7) remediate (optional, 1 retry) -> LLM patches mapping if validation/execution failed\n",
        "# 8) narrate -> LLM explains result in human language\n",
        "\n",
        "\n",
        "# Imports, keys, DB names\n",
        "\n",
        "# Loads std libs, LangGraph, and LangChain’s OpenAI wrapper\n",
        "\n",
        "# Reads OPENAI_API_KEY from env; if missing, the flow still works using deterministic fallbacks\n",
        "\n",
        "# Sets MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "# Sets SRC_DB and TGT_DB file names\n",
        "\n",
        "\n",
        "!pip -q install langgraph==0.2.39 langchain-openai==0.2.5 --upgrade\n",
        "\n",
        "import os, re, json, uuid, sqlite3, pandas as pd\n",
        "from typing import TypedDict, Optional, List, Dict, Any, Tuple\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# If using Colab secrets:\n",
        "import os\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "USE_LLM = bool(OPENAI_API_KEY)\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "SRC_DB, TGT_DB = \"source.db\", \"target.db\"\n",
        "\n",
        "\n",
        "# ---- Optional clean slate on every run (comment out if using UPSERT) ---------\n",
        "def reset_targets(db_path: str):\n",
        "    with sqlite3.connect(db_path) as con:\n",
        "        for t in [\"dim_customer\",\"fact_order\"]:\n",
        "            try: con.execute(f\"DELETE FROM {t}\")\n",
        "            except Exception: pass\n",
        "        con.commit()\n",
        "reset_targets(TGT_DB)\n",
        "\n",
        "# Your exact database setup (source.db, target.db)\n",
        "\n",
        "import os, sqlite3\n",
        "\n",
        "SRC_DB, TGT_DB = \"source.db\", \"target.db\"\n",
        "for f in [SRC_DB, TGT_DB]:\n",
        "    try: os.remove(f)\n",
        "    except FileNotFoundError: pass\n",
        "\n",
        "src_con = sqlite3.connect(SRC_DB)\n",
        "src_cur = src_con.cursor()\n",
        "src_cur.executescript(\"\"\"\n",
        "CREATE TABLE customers (\n",
        "  id INTEGER PRIMARY KEY,\n",
        "  fname TEXT,\n",
        "  lname TEXT,\n",
        "  email TEXT,\n",
        "  signup_date TEXT,\n",
        "  active_flag INTEGER\n",
        ");\n",
        "CREATE TABLE orders (\n",
        "  order_id INTEGER PRIMARY KEY,\n",
        "  customer_id INTEGER,\n",
        "  amount_cents INTEGER,\n",
        "  order_ts TEXT\n",
        ");\n",
        "\"\"\")\n",
        "src_cur.executemany(\"INSERT INTO customers VALUES (?,?,?,?,?,?)\", [\n",
        "    (1,'Amina','Alavi','amina@example.com','2023-01-15',1),\n",
        "    (2,'Sam','Lee','sam.lee@example.com','2022-11-01',1),\n",
        "    (3,'Dana','Kim','dana.kim@example.com','2021-05-20',0),\n",
        "])\n",
        "src_cur.executemany(\"INSERT INTO orders VALUES (?,?,?,?)\", [\n",
        "    (101,1,2599,'2024-09-01 12:30:00'),\n",
        "    (102,1,1099,'2024-09-15 09:00:00'),\n",
        "    (103,2, 500,'2024-09-21 18:10:00'),\n",
        "])\n",
        "src_con.commit(); src_con.close()\n",
        "\n",
        "tgt_con = sqlite3.connect(TGT_DB)\n",
        "tgt_cur = tgt_con.cursor()\n",
        "tgt_cur.executescript(\"\"\"\n",
        "CREATE TABLE dim_customer (\n",
        "  customer_id INTEGER PRIMARY KEY,\n",
        "  full_name TEXT,\n",
        "  email TEXT,\n",
        "  signup_date TEXT,\n",
        "  is_active INTEGER\n",
        ");\n",
        "CREATE TABLE fact_order (\n",
        "  order_id INTEGER PRIMARY KEY,\n",
        "  customer_id INTEGER,\n",
        "  amount_usd REAL,\n",
        "  order_date TEXT\n",
        ");\n",
        "\"\"\")\n",
        "tgt_con.commit(); tgt_con.close()\n",
        "print(\"Databases created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ7QiKmtOCmY",
        "outputId": "9b0ebea3-a77a-459d-ba9c-1987833fa246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Databases created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid LangGraph migration: agents for mapping/remediation/narration, Python for execution\n",
        "\n",
        "!pip -q install langgraph==0.2.39 langchain-openai==0.2.5 --upgrade\n",
        "\n",
        "import os, re, json, uuid, sqlite3, pandas as pd\n",
        "from typing import TypedDict, Optional, List, Dict, Any\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Optional: set your key in the environment beforehand\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "USE_LLM = bool(OPENAI_API_KEY)\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "SRC_DB, TGT_DB = \"source.db\", \"target.db\"\n",
        "\n",
        "# ------------------ Helpers (self-contained) ------------------\n",
        "# sqlite_schema_tool(db_path): introspects SQLite tables and columns into a Python dict. This is how we show the LLM what exists\n",
        "def sqlite_schema_tool(db_path: str) -> Dict[str, Any]:\n",
        "    out = {\"db\": db_path, \"tables\": {}}\n",
        "    with sqlite3.connect(db_path) as con:\n",
        "        cur = con.cursor()\n",
        "        cur.execute(\"SELECT name, sql FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'\")\n",
        "        for name, ddl in cur.fetchall():\n",
        "            cols = []\n",
        "            cur.execute(f\"PRAGMA table_info('{name}')\")\n",
        "            for _, col, ctype, notnull, dflt, pk in cur.fetchall():\n",
        "                cols.append({\"name\": col, \"type\": ctype, \"notnull\": bool(notnull), \"default\": dflt, \"pk\": bool(pk)})\n",
        "            out[\"tables\"][name] = {\"ddl\": ddl, \"columns\": cols}\n",
        "    return out\n",
        "\n",
        "# infer_mapping_with_derivations(src_schema, tgt_schema): your deterministic mapping for customers→dim_customer\n",
        "# and orders→fact_order with expressions for derived columns. This acts as a fallback if the LLM isn’t available or goes off-spec\n",
        "def infer_mapping_with_derivations(src_schema: Dict[str,Any], tgt_schema: Dict[str,Any]) -> Dict[str, Any]:\n",
        "    mapping = {\n",
        "        \"customers\": {\n",
        "            \"target\": \"dim_customer\",\n",
        "            \"columns\": [\n",
        "                {\"src\": \"id\",                 \"tgt\": \"customer_id\", \"expr\": \"id\"},\n",
        "                {\"src\": \"fname||' '||lname\",  \"tgt\": \"full_name\",   \"expr\": \"fname || ' ' || lname\"},\n",
        "                {\"src\": \"email\",              \"tgt\": \"email\",       \"expr\": \"email\"},\n",
        "                {\"src\": \"signup_date\",        \"tgt\": \"signup_date\", \"expr\": \"signup_date\"},\n",
        "                {\"src\": \"active_flag\",        \"tgt\": \"is_active\",   \"expr\": \"active_flag\"},\n",
        "            ]\n",
        "        },\n",
        "        \"orders\": {\n",
        "            \"target\": \"fact_order\",\n",
        "            \"columns\": [\n",
        "                {\"src\": \"order_id\",     \"tgt\": \"order_id\",   \"expr\": \"order_id\"},\n",
        "                {\"src\": \"customer_id\",  \"tgt\": \"customer_id\",\"expr\": \"customer_id\"},\n",
        "                {\"src\": \"amount_cents\", \"tgt\": \"amount_usd\", \"expr\": \"CAST(amount_cents AS REAL) / 100.0\"},\n",
        "                {\"src\": \"order_ts\",     \"tgt\": \"order_date\", \"expr\": \"substr(order_ts, 1, 10)\"},\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "    for s_table, m in list(mapping.items()):\n",
        "        tgt = m[\"target\"]\n",
        "        mapping[s_table][\"create_target\"] = tgt not in tgt_schema[\"tables\"]\n",
        "    return {\"proposed_mapping\": mapping}\n",
        "\n",
        "# propose_plan_tool(src_schema, tgt_schema, mapping): turns a mapping into a step list. For each source table:\n",
        "#  [CREATE_TABLE if needed, TRANSFER, VALIDATE_COUNTS, CHECKSUM]. Returns {\"plan_id\",\"steps\",\"mapping\"}\n",
        "def propose_plan_tool(src_schema: Dict[str,Any], tgt_schema: Dict[str,Any], mapping: Dict[str,Any]) -> Dict[str, Any]:\n",
        "    steps = []\n",
        "    mp = mapping.get(\"proposed_mapping\", mapping)\n",
        "    for s_table, m in mp.items():\n",
        "        tgt = m.get(\"target\")\n",
        "        if m.get(\"create_target\", False) or tgt not in tgt_schema[\"tables\"]:\n",
        "            steps.append({\"action\":\"CREATE_TABLE\", \"source_table\": s_table, \"target_table\": tgt})\n",
        "        steps.append({\"action\":\"TRANSFER\", \"source_table\": s_table, \"target_table\": tgt})\n",
        "        steps.append({\"action\":\"VALIDATE_COUNTS\", \"source_table\": s_table, \"target_table\": tgt})\n",
        "        steps.append({\"action\":\"CHECKSUM\", \"source_table\": s_table, \"target_table\": tgt, \"columns\":\"*\"})\n",
        "    import uuid as _uuid\n",
        "    return {\"plan_id\": str(_uuid.uuid4()), \"steps\": steps, \"mapping\": mp}\n",
        "\n",
        "# generate_sql_tool(src_db, tgt_db, step, src_schema, mapping):\n",
        "# For CREATE_TABLE: builds a target DDL from the mapping and simple type rules\n",
        "# For TRANSFER: emits an ATTACH/INSERT/SELECT/DETACH script that pulls from src and inserts into tgt with the mapping exprs\n",
        "\n",
        "def generate_sql_tool(src_db: str, tgt_db: str, step: dict, src_schema: dict, mapping: dict) -> dict:\n",
        "    action = step[\"action\"]\n",
        "    if action == \"CREATE_TABLE\":\n",
        "        s_table = step[\"source_table\"]; t_table = step[\"target_table\"]\n",
        "        col_map = mapping[s_table][\"columns\"]\n",
        "        src_cols = {c[\"name\"]: (c[\"type\"] or \"TEXT\") for c in src_schema[\"tables\"][s_table][\"columns\"]}\n",
        "        def guess_type(tgt_col, src_expr):\n",
        "            if tgt_col.lower().endswith(\"_id\") or tgt_col in (\"order_id\",\"customer_id\",\"is_active\"):\n",
        "                return \"INTEGER\"\n",
        "            if tgt_col == \"amount_usd\":\n",
        "                return \"REAL\"\n",
        "            return src_cols.get(src_expr, \"TEXT\") if src_expr in src_cols else \"TEXT\"\n",
        "        col_defs = []\n",
        "        pk = \"customer_id\" if t_table == \"dim_customer\" else (\"order_id\" if t_table == \"fact_order\" else None)\n",
        "        for cm in col_map:\n",
        "            tgt_col = cm[\"tgt\"]\n",
        "            typ = guess_type(tgt_col, cm.get(\"src\") or cm.get(\"expr\",\"\"))\n",
        "            if pk and tgt_col == pk:\n",
        "                col_defs.append(f\"{tgt_col} {typ} PRIMARY KEY\")\n",
        "            else:\n",
        "                col_defs.append(f\"{tgt_col} {typ}\")\n",
        "        ddl = f\"CREATE TABLE IF NOT EXISTS {t_table} (\\n  \" + \",\\n  \".join(col_defs) + \"\\n)\"\n",
        "        return {\"sql\": ddl}\n",
        "\n",
        "    if action == \"TRANSFER\":\n",
        "        s_table = step[\"source_table\"]; t_table = step[\"target_table\"]\n",
        "        col_map = mapping[s_table][\"columns\"]\n",
        "        tgt_cols = [cm[\"tgt\"] for cm in col_map]\n",
        "        selects = [cm.get(\"expr\") or cm[\"src\"] for cm in col_map]\n",
        "        sql = f\"\"\"\n",
        "ATTACH DATABASE '{src_db}' AS src;\n",
        "INSERT INTO {t_table} ({', '.join(tgt_cols)})\n",
        "SELECT {', '.join(selects)} FROM src.{s_table};\n",
        "DETACH DATABASE src;\n",
        "\"\"\"\n",
        "        return {\"sql\": sql}\n",
        "\n",
        "    if action in (\"VALIDATE_COUNTS\",\"CHECKSUM\"):\n",
        "        return {\"sql\": None}\n",
        "    return {\"sql\": None}\n",
        "\n",
        "# execute_sql_tool(db_path, sql): runs executescript against the target DB so multi-statement SQL works\n",
        "def execute_sql_tool(db_path: str, sql: str) -> dict:\n",
        "    with sqlite3.connect(db_path) as con:\n",
        "        cur = con.cursor()\n",
        "        cur.executescript(sql)\n",
        "        con.commit()\n",
        "    return {\"ok\": True}\n",
        "\n",
        "# validate_counts_tool and checksum_tool: basic validation primitives used later\n",
        "def validate_counts_tool(src_db: str, tgt_db: str, s_table: str, t_table: str) -> Dict[str, Any]:\n",
        "    with sqlite3.connect(src_db) as cs, sqlite3.connect(tgt_db) as ct:\n",
        "        src_cnt = pd.read_sql_query(f\"SELECT COUNT(*) AS n FROM {s_table}\", cs)[\"n\"].iloc[0]\n",
        "        try:\n",
        "            tgt_cnt = pd.read_sql_query(f\"SELECT COUNT(*) AS n FROM {t_table}\", ct)[\"n\"].iloc[0]\n",
        "        except Exception:\n",
        "            tgt_cnt = None\n",
        "    return {\"src_count\": int(src_cnt), \"tgt_count\": None if tgt_cnt is None else int(tgt_cnt),\n",
        "            \"match\": tgt_cnt is not None and int(src_cnt)==int(tgt_cnt)}\n",
        "\n",
        "def checksum_tool(db_path: str, table: str, cols: Optional[List[str]]=None) -> Dict[str, Any]:\n",
        "    if cols is None or cols==\"*\" or cols==[\"*\"]:\n",
        "        with sqlite3.connect(db_path) as con:\n",
        "            df = pd.read_sql_query(f\"PRAGMA table_info('{table}')\", con)\n",
        "            cols = df[\"name\"].tolist()\n",
        "    expr = \" || '|' || \".join([f\"IFNULL({c},'NULL')\" for c in cols])\n",
        "    with sqlite3.connect(db_path) as con:\n",
        "        q = f\"SELECT SUM(LENGTH({expr})) AS len_sum FROM {table}\"\n",
        "        len_sum = pd.read_sql_query(q, con)[\"len_sum\"].iloc[0] or 0\n",
        "    return {\"columns\": cols, \"len_sum\": int(len_sum)}\n",
        "\n",
        "# Optional: clean target tables so reruns don’t hit PK collisions\n",
        "def reset_targets(db_path: str):\n",
        "    with sqlite3.connect(db_path) as con:\n",
        "        for t in [\"dim_customer\", \"fact_order\"]:\n",
        "            try: con.execute(f\"DELETE FROM {t}\")\n",
        "            except Exception: pass\n",
        "        con.commit()\n",
        "\n",
        "reset_targets(TGT_DB)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ Tiny LLM utilities ------------------\n",
        "# llm_json(prompt, default): calls the model and tries to parse the first JSON object\n",
        "# in the reply. If there’s no key or bad JSON, returns default\n",
        "def llm_json(prompt: str, default: Any) -> Any:\n",
        "    if not USE_LLM:\n",
        "        return default\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=MODEL, temperature=0.2, openai_api_key=OPENAI_API_KEY)\n",
        "    txt = llm.invoke(prompt).content\n",
        "    m = re.search(r\"\\{[\\s\\S]*\\}\", txt)\n",
        "    if not m:\n",
        "        return default\n",
        "    try:\n",
        "        return json.loads(m.group(0))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "# llm_text(prompt): calls the model and returns the text\n",
        "def llm_text(prompt: str) -> str:\n",
        "    if not USE_LLM:\n",
        "        return \"(narration skipped: no OPENAI_API_KEY in environment)\"\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=MODEL, temperature=0.2, openai_api_key=OPENAI_API_KEY)\n",
        "    return llm.invoke(prompt).content"
      ],
      "metadata": {
        "id": "lGh4tnOLbJGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ------------------ Graph state and nodes ------------------\n",
        "# The state type (MigState)\n",
        "# This is the working memory the graph passes through nodes\n",
        "# Keys you will look at:\n",
        "# mapping: the mapping currently in use\n",
        "# plan: the plan with steps\n",
        "# sql_blocks: the generated SQLs that were actually executed\n",
        "# exec_log and exec_errors: what ran and any errors\n",
        "# validations: count and checksum results\n",
        "# narration: agent-written summary at the end\n",
        "# attempt: retry counter\n",
        "# Nodes (this is where the work happens)\n",
        "\n",
        "class MigState(TypedDict):\n",
        "    plan_id: str                     # 🆔 A unique ID for this migration run (like a project code)\n",
        "    attempt: int                     # 🔁 How many times we’ve tried running this plan (helps with retries)\n",
        "    src_schema: Dict[str, Any]       # 📜 Structure of the source database (tables, columns, etc.)\n",
        "    tgt_schema: Dict[str, Any]       # 📜 Structure of the target database\n",
        "    mapping: Dict[str, Any]          # 🧭 The “map” showing how source tables/columns match to target ones\n",
        "    plan: Dict[str, Any]             # 🪄 The actual migration plan (list of steps: create table, transfer data, etc.)\n",
        "    sql_blocks: List[Dict[str, Any]] # 📦 The SQL commands we generated to run each step\n",
        "    exec_log: List[Dict[str, Any]]   # 🧰 A history of what SQL statements were executed and their results\n",
        "    exec_errors: List[str]           # 🚨 Any errors that happened during execution\n",
        "    validations: List[Dict[str, Any]]# ✅ Results of checks like row counts and checksums\n",
        "    narration: Optional[str]         # 🗣️ A plain-English summary of what happened (agents write this)\n",
        "\n",
        "# 🧱 node_schema reads the structure (schema) of both the source and target databases.\n",
        "# It saves what tables and columns exist in each one into MigState.\n",
        "# Later steps use this info to plan how data should be transferred.\n",
        "def node_schema(state: MigState) -> MigState:\n",
        "    state[\"src_schema\"] = sqlite_schema_tool(SRC_DB)\n",
        "    state[\"tgt_schema\"] = sqlite_schema_tool(TGT_DB)\n",
        "    return state\n",
        "\n",
        "# 🗺️ node_mapplan figures out how tables and columns from the source match up with the target.\n",
        "# It uses that mapping to build a detailed migration plan — like a checklist of what needs to happen.\n",
        "# The plan includes steps like CREATE_TABLE, TRANSFER, VALIDATE_COUNTS, and CHECKSUM.\n",
        "def node_mapplan(state: MigState) -> MigState:\n",
        "    fallback = infer_mapping_with_derivations(state[\"src_schema\"], state[\"tgt_schema\"])[\"proposed_mapping\"]\n",
        "    prompt = f\"\"\"\n",
        "You are a data migration agent. Propose mapping JSON in this structure:\n",
        "{{ \"<source_table>\": {{ \"target\":\"<target_table>\", \"columns\":[{{\"src\":\"...\", \"tgt\":\"...\", \"expr\":\"...\"}}] }} }}\n",
        "Use valid SQLite expressions. Include derived fields where needed.\n",
        "Source schema:\n",
        "{json.dumps(state[\"src_schema\"], indent=2)}\n",
        "Target schema:\n",
        "{json.dumps(state[\"tgt_schema\"], indent=2)}\n",
        "\"\"\"\n",
        "    proposed = llm_json(prompt, default=fallback)\n",
        "    state[\"mapping\"] = proposed if isinstance(proposed, dict) and proposed else fallback\n",
        "    return state\n",
        "\n",
        "def node_plan(state: MigState) -> MigState:\n",
        "    plan = propose_plan_tool(state[\"src_schema\"], state[\"tgt_schema\"], {\"proposed_mapping\": state[\"mapping\"]})\n",
        "    state[\"plan\"] = plan\n",
        "    return state\n",
        "\n",
        "# 🧪 node_sqlgen generates the actual SQL commands for each step in the plan.\n",
        "# Example: it creates CREATE TABLE statements and INSERT ... SELECT queries for data transfer.\n",
        "# These SQL statements will later be executed to perform the migration.\n",
        "def node_sqlgen(state: MigState) -> MigState:\n",
        "    blocks = []\n",
        "    for i, step in enumerate(state[\"plan\"][\"steps\"], 1):\n",
        "        g = generate_sql_tool(SRC_DB, TGT_DB, step, state[\"src_schema\"], state[\"plan\"][\"mapping\"])\n",
        "        if g.get(\"sql\"):\n",
        "            blocks.append({\"i\": i, \"action\": step[\"action\"], \"sql\": g[\"sql\"],\n",
        "                           \"source_table\": step.get(\"source_table\"), \"target_table\": step.get(\"target_table\")})\n",
        "    state[\"sql_blocks\"] = blocks\n",
        "    return state\n",
        "\n",
        "# ⚙️ node_execute runs the SQL statements created earlier against the target database.\n",
        "# It also performs validation checks — like counting rows and verifying checksums — to make sure\n",
        "# the data transferred correctly. All results (successes or errors) are saved into MigState.\n",
        "def node_execute(state: MigState) -> MigState:\n",
        "    state[\"exec_log\"] = []\n",
        "    state[\"exec_errors\"] = []\n",
        "    for b in state[\"sql_blocks\"]:\n",
        "        if b[\"action\"] not in (\"CREATE_TABLE\",\"TRANSFER\"):\n",
        "            continue\n",
        "        try:\n",
        "            execute_sql_tool(TGT_DB, b[\"sql\"])\n",
        "            state[\"exec_log\"].append({\"i\": b[\"i\"], \"action\": b[\"action\"]})\n",
        "        except Exception as e:\n",
        "            state[\"exec_errors\"].append(f\"Step {b['i']} {b['action']} error: {e}\")\n",
        "    return state\n",
        "\n",
        "def _all_ok(vs: List[Dict[str,Any]]) -> bool:\n",
        "    ok = True\n",
        "    for v in vs:\n",
        "        if v[\"type\"] == \"counts\" and not v.get(\"match\", False): ok = False\n",
        "        if v[\"type\"] == \"checksum\" and not v.get(\"match\", False): ok = False\n",
        "    return ok\n",
        "\n",
        "def node_validate(state: MigState) -> MigState:\n",
        "    reports = []\n",
        "    for step in state[\"plan\"][\"steps\"]:\n",
        "        if step[\"action\"] == \"VALIDATE_COUNTS\":\n",
        "            r = validate_counts_tool(SRC_DB, TGT_DB, step[\"source_table\"], step[\"target_table\"])\n",
        "            reports.append({\"type\":\"counts\",\"src\":step[\"source_table\"],\"tgt\":step[\"target_table\"], **r})\n",
        "        if step[\"action\"] == \"CHECKSUM\":\n",
        "            s = checksum_tool(SRC_DB, step[\"source_table\"])\n",
        "            t = checksum_tool(TGT_DB, step[\"target_table\"])\n",
        "            reports.append({\"type\":\"checksum\",\"src\":step[\"source_table\"],\"tgt\":step[\"target_table\"],\n",
        "                            \"src_len_sum\": s[\"len_sum\"], \"tgt_len_sum\": t[\"len_sum\"], \"match\": s[\"len_sum\"]==t[\"len_sum\"]})\n",
        "    state[\"validations\"] = reports\n",
        "    return state\n",
        "\n",
        "def node_remediate(state: MigState) -> MigState:\n",
        "    if state[\"attempt\"] >= 1:\n",
        "        return state\n",
        "    bad = (len(state[\"exec_errors\"]) > 0) or (not _all_ok(state[\"validations\"]))\n",
        "    if not bad:\n",
        "        return state\n",
        "    prompt = f\"\"\"\n",
        "Execution or validation failed. Patch the mapping so it works.\n",
        "Errors:\n",
        "{json.dumps(state[\"exec_errors\"], indent=2)}\n",
        "Validations:\n",
        "{json.dumps(state[\"validations\"], indent=2)}\n",
        "Current mapping:\n",
        "{json.dumps(state[\"mapping\"], indent=2)}\n",
        "Source schema:\n",
        "{json.dumps(state[\"src_schema\"], indent=2)}\n",
        "Target schema:\n",
        "{json.dumps(state[\"tgt_schema\"], indent=2)}\n",
        "Return only a JSON mapping object: {{ \"<source_table>\": {{ \"target\":\"...\", \"columns\":[{{\"src\":\"...\",\"tgt\":\"...\",\"expr\":\"...\"}}] }} }}\n",
        "\"\"\"\n",
        "    patched = llm_json(prompt, default=state[\"mapping\"])\n",
        "    state[\"mapping\"] = patched if isinstance(patched, dict) and patched else state[\"mapping\"]\n",
        "    state[\"attempt\"] += 1\n",
        "    return state\n",
        "\n",
        "# 📜 node_narrate writes a simple human-readable summary of everything that happened:\n",
        "# how the migration went, what steps ran, if validations passed, and if the plan succeeded.\n",
        "# This is helpful for debugging or reporting what the agents did.\n",
        "def node_narrate(state: MigState) -> MigState:\n",
        "    status = \"OK\" if (len(state[\"exec_errors\"])==0 and _all_ok(state[\"validations\"])) else \"FAIL\"\n",
        "    summary = {\n",
        "        \"plan_id\": state[\"plan_id\"],\n",
        "        \"attempts\": state[\"attempt\"],\n",
        "        \"exec_log\": state[\"exec_log\"],\n",
        "        \"exec_errors\": state[\"exec_errors\"],\n",
        "        \"validations\": state[\"validations\"],\n",
        "        \"final_status\": status\n",
        "    }\n",
        "    prompt = (\n",
        "        \"You are the Coordinator. Summarize the migration in <=160 words using ONLY this JSON. \"\n",
        "        \"End with 'STATUS: OK' or 'STATUS: FAIL'.\\n\\n\" + json.dumps(summary, indent=2)\n",
        "    )\n",
        "    state[\"narration\"] = llm_text(prompt)\n",
        "    return state"
      ],
      "metadata": {
        "id": "8sKBSPrwbS-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ------------------ Graph wiring ------------------\n",
        "\n",
        "graph = StateGraph(MigState)\n",
        "graph.add_node(\"get_schema\",      node_schema)\n",
        "graph.add_node(\"propose_mapping\", node_mapplan)\n",
        "graph.add_node(\"build_plan\",      node_plan)\n",
        "graph.add_node(\"gen_sql\",         node_sqlgen)\n",
        "graph.add_node(\"run_sql\",         node_execute)\n",
        "graph.add_node(\"do_validate\",     node_validate)\n",
        "graph.add_node(\"remediate\",       node_remediate)\n",
        "graph.add_node(\"narrate\",         node_narrate)\n",
        "\n",
        "graph.set_entry_point(\"get_schema\")\n",
        "graph.add_edge(\"get_schema\", \"propose_mapping\")\n",
        "graph.add_edge(\"propose_mapping\", \"build_plan\")\n",
        "graph.add_edge(\"build_plan\", \"gen_sql\")\n",
        "graph.add_edge(\"gen_sql\", \"run_sql\")\n",
        "graph.add_edge(\"run_sql\", \"do_validate\")\n",
        "\n",
        "def after_validate(state: MigState) -> str:\n",
        "    bad = (len(state[\"exec_errors\"]) > 0) or (not _all_ok(state[\"validations\"]))\n",
        "    return \"remediate\" if (bad and state[\"attempt\"] < 1) else \"narrate\"\n",
        "\n",
        "graph.add_conditional_edges(\"do_validate\", after_validate, {\n",
        "    \"remediate\": \"remediate\",\n",
        "    \"narrate\": \"narrate\"\n",
        "})\n",
        "graph.add_edge(\"remediate\", \"build_plan\")\n",
        "graph.add_edge(\"narrate\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "4q406Dn4bVsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ------------------ Run once ------------------\n",
        "\n",
        "initial: MigState = {\n",
        "    \"plan_id\": str(uuid.uuid4()),\n",
        "    \"attempt\": 0,\n",
        "    \"src_schema\": {},\n",
        "    \"tgt_schema\": {},\n",
        "    \"mapping\": {},\n",
        "    \"plan\": {},\n",
        "    \"sql_blocks\": [],\n",
        "    \"exec_log\": [],\n",
        "    \"exec_errors\": [],\n",
        "    \"validations\": [],\n",
        "    \"narration\": None\n",
        "}"
      ],
      "metadata": {
        "id": "TXlvTeDDbYMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_state = app.invoke(initial)\n",
        "\n",
        "print(\"Plan ID:\", final_state[\"plan_id\"])\n",
        "print(\"Attempts:\", final_state[\"attempt\"])\n",
        "print(\"Executed statements:\", len(final_state[\"exec_log\"]))\n",
        "print(\"Exec errors:\", final_state[\"exec_errors\"])\n",
        "print(\"Validations:\")\n",
        "for v in final_state[\"validations\"]:\n",
        "    print(\" \", v)\n",
        "print(\"\\nNarration:\\n\", final_state[\"narration\"])\n",
        "\n",
        "with sqlite3.connect(TGT_DB) as con:\n",
        "    print(\"\\n-- dim_customer\")\n",
        "    print(pd.read_sql_query(\"SELECT * FROM dim_customer ORDER BY customer_id\", con))\n",
        "    print(\"\\n-- fact_order\")\n",
        "    print(pd.read_sql_query(\"SELECT * FROM fact_order ORDER BY order_id\", con))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y1f0-ACbDeX",
        "outputId": "dba12668-40cc-45bc-93d9-08b7dc79a8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plan ID: 7ec13e78-1663-4f39-83a2-074a315420cb\n",
            "Attempts: 1\n",
            "Executed statements: 0\n",
            "Exec errors: ['Step 1 TRANSFER error: UNIQUE constraint failed: dim_customer.customer_id', 'Step 4 TRANSFER error: UNIQUE constraint failed: fact_order.order_id']\n",
            "Validations:\n",
            "  {'type': 'counts', 'src': 'customers', 'tgt': 'dim_customer', 'src_count': 3, 'tgt_count': 3, 'match': True}\n",
            "  {'type': 'checksum', 'src': 'customers', 'tgt': 'dim_customer', 'src_len_sum': 130, 'tgt_len_sum': 130, 'match': True}\n",
            "  {'type': 'counts', 'src': 'orders', 'tgt': 'fact_order', 'src_count': 3, 'tgt_count': 3, 'match': True}\n",
            "  {'type': 'checksum', 'src': 'orders', 'tgt': 'fact_order', 'src_len_sum': 89, 'tgt_len_sum': 64, 'match': False}\n",
            "\n",
            "Narration:\n",
            " (narration skipped: no OPENAI_API_KEY in environment)\n",
            "\n",
            "-- dim_customer\n",
            "   customer_id    full_name                 email signup_date  is_active\n",
            "0            1  Amina Alavi     amina@example.com  2023-01-15          1\n",
            "1            2      Sam Lee   sam.lee@example.com  2022-11-01          1\n",
            "2            3     Dana Kim  dana.kim@example.com  2021-05-20          0\n",
            "\n",
            "-- fact_order\n",
            "   order_id  customer_id  amount_usd  order_date\n",
            "0       101            1       25.99  2024-09-01\n",
            "1       102            1       10.99  2024-09-15\n",
            "2       103            2        5.00  2024-09-21\n"
          ]
        }
      ]
    }
  ]
}